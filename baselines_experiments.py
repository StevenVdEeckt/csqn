# -*- coding: utf-8 -*-
"""CIFAR10_AlexNet_ClassIncremental_experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Xgk1i0qcftWAvqeuhETc8QnOWI3KtQJ
"""

import torch
from baselines import *
from train_functions import *
import model
from dataset import *
import logging
import argparse

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


parser = argparse.ArgumentParser(description='Five dataset experiments..')
parser.add_argument('--ewc', type=int, default=1, help='Indicates if EWC should be run (1) or not (0)')
parser.add_argument('--ogd', type=int, default=1, help='Indicates if OGD should be run (1) or not (0)')
parser.add_argument('--lwf', type=int, default=1, help='Indicates if LWF should be run (1) or not (0)')
parser.add_argument('--mas', type=int, default=1, help='Indicates if MAS should be run (1) or not (0)')
parser.add_argument('--ft', type=int, default=0, help='Indicates if Fine-Tuning should be run (1) or not (0)')
parser.add_argument('--start', type=int, default=1, help='ID of first iteration')
parser.add_argument('--iter', type=int, default=3, help='Number of iterations to run each method')
parser.add_argument('--log_id', type=int, default=0, help="Log id for log file - if -1, output written to Terminal")

args = parser.parse_args()

FORMAT = '%(asctime)-15s %(message)s'
if args.log_id == -1:
    logging.basicConfig(level=logging.DEBUG, format=FORMAT, datefmt='%m/%d/%Y %I:%M:%S %p')
else:
    logging.basicConfig(filename='5resnet_exp_%d.log' % args.log_id, level=logging.DEBUG, format=FORMAT, datefmt='%m/%d/%Y %I:%M:%S %p')
logger = logging.getLogger('main')


""" Hyper-parameter search settings """
p_hyp = 0.8  # method must at least reach p*ACC_FT on new task
a_hyp = 0.5  # decaying factor


""" The baselines and their settings """
run = []
if args.ft == 1:
    run.append('Fine-Tuning')
if args.ewc == 1:
    run.append('EWC')
if args.ogd == 1:
    run.append('OGD')
if args.lwf == 1:
    run.append('LWF')
if args.mas == 1:
    run.append('MAS')
if len(run) == 0:
    raise Exception("Exception: no methods to run!")
start_id = {'EWC': args.start, 'MAS': args.start, 'OGD': args.start, 'LWF': args.start, 'Fine-Tuning': 1}
iterations = {'EWC': args.iter, 'MAS': args.iter, 'OGD': args.iter, 'LWF': args.iter, 'Fine-Tuning': 1}
lambdas = {'EWC': [1, 100, 1000, 10000, 100000],
           'MAS': [1, 10, 100, 1000, 10000],
           'LWF': [0.01, 0.1, 1, 10, 100]}
T_CV = 5
opt_alpha = {}

""" Experimental settings """
epochs = {0: 50, 1: 50, 2: 50, 3: 50, 4: 50}
batch_size = 128
n_tasks = 5
n_classes = 10
neural_net = 'lenet'
res_name = 'five_dataset_results'
first_task = 'first_task_resnet_five'

""" The data """
train_data, dev_data, test_data, sample_data = get_five_datasets(batch_size=batch_size)
# required for OGD
train_data1, _, _, _ = get_five_datasets(batch_size=1)

""" The neural network """
get_net = lambda: model.get_net(neural_net, [n_classes] * (n_tasks))
net, shared_layers = model.get_net(neural_net,  [n_classes] * (n_tasks), return_shared_layers=True)

logger.debug("Neural network contains " + str(torch.cat([p.view(-1) for p in list(net.parameters())]).numel())
              + " parameters.")
for n, p in net.state_dict().items():
    logger.debug(n + " ---> %d parameters" % (p.numel()))

logger.debug('Following are the shared layers: %s', str(shared_layers))

init(train_data, dev_data, test_data, res_name, n_tasks, epochs[0], n_classes, shared_layers)

net_one = get_net()
try:
    net_one.load_state_dict(torch.load(first_task))
except Exception as e:
    logger.warning('Exception: file %s did not exist - training on task one.. (exception was = %s)' % (first_task, e))
    train_net(net_one, 0, epochs=epochs[0], eval=True)
    torch.save(net_one.state_dict(), first_task)

logger.info("")
logger.info("# 3 Experiments with the Baselines")

for method in run:

    logger.info("Method: %s" % method)

    for num in range(start_id[method], start_id[method] + iterations[method]):

        mod_list = [copy.deepcopy(net_one).state_dict()]

        logger.info("### RUN %d of %d " % (num, iterations[method]))

        if method == 'EWC':
            regulator = EWC(shared_layers=shared_layers, n_classes=n_classes)
        elif method == 'MAS':
            regulator = MAS(shared_layers=shared_layers)
        elif method == 'LWF':
            regulator = LWF()
        elif method == 'OGD':
            regulator = OGD(M=200, shared_layers=shared_layers, n_classes=n_classes)

        for task in range(n_tasks - 1):

            logger.info("Adapting Task %d to Task %d" % (task, task + 1))
            net_ = get_net()
            net_.load_state_dict(mod_list[-1])
            names, accs = [('Task %d' % i) for i in range(task + 1)], [round(test(net_, i, 'dev', print_result=False), 2)
                                                                       for i in range(task + 1)]
            logger.debug('Initial model: ' + str(names) + " = " + str(accs))

            if method == 'EWC':
                regulator.compute_FIM(net_, sample_data(task), task)
            elif method == 'MAS':
                regulator.compute_IW(net_, sample_data(task), task)
            elif method == 'LWF':
                logger.debug('LWF - stage 1: training without shared layers')
                train_net(net_, task+1, epochs=epochs[task+1] // 5, freeze_layers=shared_layers)
                regulator.set_old_net(net_)
                sd_frozen = {n: copy.deepcopy(p) for n, p in net_.state_dict().items()}
            elif method == 'OGD':
                regulator.compute_gradients(net_, train_data1(task), task)

            if method != 'OGD':
                logger.debug("Adapting without regularization..")
                net_ = get_net()
                net_.load_state_dict(mod_list[-1])
                train_net(net_, task + 1, epochs=epochs[task+1])
                acc_ft = test(net_, task + 1, 'dev')
                if method == 'Fine-Tuning':
                    mod_list.append(net_.state_dict())
                    continue

                acc, alpha = 0, init_alpha[method]

                while acc < p_hyp * acc_ft:
                    logger.debug('alpha = %s' % alpha)
                    net_ = get_net()
                    if method == 'LWF':
                        net_.load_state_dict(sd_frozen)
                        train_net(net_, task + 1, reg_loss=lambda x, y, t: regulator.regularize(x, y, alpha, t), epochs=epochs[task+1])
                    else:
                        net_.load_state_dict(mod_list[-1])
                        names, accs = [('Task %d' % i) for i in range(task + 1)], [
                            round(test(net_, i, 'dev', print_result=False), 2)
                            for i in range(task + 1)]
                        logger.info('Initial model: ' + str(names) + " = " + str(accs))
                        train_net(net_, task + 1, reg_loss=lambda x: regulator.regularize(x, alpha), epochs=epochs[task+1])
                    acc = test(net_, task + 1, 'dev')
                    logger.info("[Task %d] = [%.2f]" % (task + 1, acc))
                    alpha = a_hyp * alpha

                best_alpha = alpha / a_hyp
                logger.info("Best alpha was %d - with an average accuracy of %.2f" % (best_alpha, acc))

                mod_list.append(net_.state_dict())

            else:
                net_.load_state_dict(mod_list[-1])
                train_net(net_, task + 1, grad_fn=lambda x: regulator.regularize(x), epochs=n_epoch, opt='sgd')
                mod_list.append(net_.state_dict())

        logger.info("## 3.3 Evaluation")
        net_list = []
        for mod in mod_list:
            net_ = get_net().cpu()
            net_.load_state_dict(mod)
            net_list.append(net_)
        if method == 'Fine-Tuning':
            test_and_update(net_list, method)
        else:
            test_and_update(net_list, method + ' %d' % num)
