# -*- coding: utf-8 -*-
"""Rotated_MNIST_experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BsYrz6Dx8s8ou1xDaByVYgalvS7fk9rk
"""

import torch
import argparse
import logging
import csqn, baselines
import model
from train_functions import *
from dataset import *
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



def cuda_overview():
    t = torch.cuda.get_device_properties(0).total_memory * 10.0 ** (-9)
    r = torch.cuda.memory_reserved(0) * 10.0 ** (-9)
    a = torch.cuda.memory_allocated(0) * 10.0 ** (-9)
    f = r - a  # free inside reserved
    logging.debug("CUDA memory - total: %.4f; reserved: %.4f; allocated: %.4f; available: %.4f" % (t, r, a, f))


parser = argparse.ArgumentParser(description='Continual Learning experiments..')
parser.add_argument('--experiments', type=int, default=0,
                    help="Choose 0 for Rotated MNIST (default), 1 for Split CIFAR-10/100, 2 for Five Datasets")
parser.add_argument('--method', type=str, default='Fine-Tuning',
                     help="Select the CL method. Choose from Fine-Tuning (default), EWC, MAS, LWF, OGD and "
                          "CSQN-X (M) with X = S or B and M a positive integer. "
                          "Optionally, you can add to CSQN-X (M) R with R in [2N, EV, CT, SPL] as reduction strategy.")
parser.add_argument('--start', type=int, default=1, help='ID of first iteration')
parser.add_argument('--iter', type=int, default=3, help='Number of iterations')
parser.add_argument('--tasks', type=int, default=20,
                    help='Number of tasks to run. Only used when Rotated MNIST experiments are selected')
parser.add_argument('--angle', type=int, default=5,
                    help='Angle of the rotation. Only used when Rotated MNIST experiments are selected')
parser.add_argument('--log_file', type=str, default='log',
                    help='Name of the log file - default is just log. If T, results are written to Terminal')
parser.add_argument('--p_hyp', type=float, default=0.8, help='p parameter in methodology of De Lange et al'
                                                             'for choosing hyper-parameters.')
parser.add_argument('--a_hyp', type=float, default=0.5, help='alpha parameter in methodology of De Lange et al'
                                                             'for choosing hyper-parameters.')
args = parser.parse_args()

""" Logging """
FORMAT = '%(asctime)-15s %(message)s'
if args.log_file == 'T':
    logging.basicConfig(format=FORMAT,  level=logging.DEBUG, datefmt='%m/%d/%Y %I:%M:%S %p')
else:
    logging.basicConfig(filename=args.log_file, format=FORMAT,  level=logging.DEBUG, datefmt='%m/%d/%Y %I:%M:%S %p')
logger = logging.getLogger('main')

logging.info("Set the device: %s", str(device))


""" Hyper-parameter search settings """
p_hyp = args.p_hyp  # method must at least reach p*ACC_FT on new task
a_hyp = args.a_hyp  # decaying factor
logger.debug("Methodology of De Lange et al., 2019 with p = %s, alpha = %s" % (str(p_hyp), str(a_hyp)))
init_alpha = {'EWC': 1e8, 'CSQN': 1e6, 'MAS': 1e5, 'LWF': 1e2}

""" Number of runs """
start_id, iterations = args.start, args.iter

""" Load the data and the network. Initialize train_functions """
if args.experiments == 0:
    n_tasks, offset, batch_size, n_epoch = args.tasks, args.angle, 64, 10
    train_data, dev_data, test_data = get_rotated_mnist_data(n_tasks=n_tasks, angle=offset, batch_size=batch_size)
    train_data1, _, _ = get_rotated_mnist_data(n_tasks=n_tasks, angle=offset, batch_size=1)
    get_net = lambda: model.get_net('mlp', 10)
    shared_layers = None
    first_task = 'task1_rotmnist_mlp'
    res_name = 'rotated_mnist_results_%d_%d_lange' % (offset, n_tasks)
    init(train_data, dev_data, test_data, res_name, n_tasks, n_epoch)
    n_classes = None
    sample_data = train_data
    first_epoch = n_epoch
elif args.experiments == 1:
    n_tasks, batch_size, n_epoch = 11, 64, 10
    train_data, dev_data, test_data, n_classes = get_split_cifar10_100_data(n_tasks=n_tasks-1, batch_size=batch_size)
    train_data1, _, _, _ = get_split_cifar10_100_data(n_tasks=n_tasks-1, batch_size=1)
    _, shared_layers = model.get_net('resnet', [n_classes] * (n_tasks + 1), return_shared_layers=True)
    get_net = lambda: model.get_net('resnet', [n_classes] * (n_tasks + 1))
    res_name = 'cifar10100_resnet_results'
    first_task = 'task1_resnet_cifar10'
    init(train_data, dev_data, test_data, res_name, n_tasks, n_epoch, n_classes, shared_layers)
    sample_data = train_data
    first_epoch = 15
elif args.experiments == 2:
    batch_size, n_epoch, n_tasks = 128, 50, 5
    n_classes = 10
    train_data, dev_data, test_data, sample_data = get_five_datasets(batch_size=batch_size)
    train_data1, _, _, _ = get_five_datasets(batch_size=1)
    get_net = lambda: model.get_net('lenet', [n_classes] * n_tasks)
    _, shared_layers = model.get_net('lenet', [n_classes] * (n_tasks), return_shared_layers=True)
    res_name = 'five_dataset_results'
    first_task = 'first_task_resnet_five'
    init(train_data, dev_data, test_data, res_name, n_tasks, n_epoch, n_classes, shared_layers)
    first_epoch = n_epoch
else:
    raise Exception('Please choose 0, 1 or 2 for --experiments')

net = get_net()
logging.debug("Neural network contains " + str(torch.cat([p.view(-1) for p in list(net.parameters())]).numel())
      + " parameters.")
for n, p in net.state_dict().items():
    logging.debug(n + " ---> %d parameters" % (p.numel()))


net_one = get_net()
try:
    net_one.load_state_dict(torch.load(first_task))
except Exception as e:
    logger.warning('Exception: file %s did not exist - training on task one.. (exception was = %s)' % (first_task, e))
    train_net(net_one, 0, epochs=first_epoch, eval=True)
    torch.save(net_one.state_dict(), first_task)


""" Determining the method.. """
if 'CSQN' in args.method:
    method = 'CSQN'
    qnm = 'BFGS' if args.method.split(' ')[0].split('-')[-1] == 'B' else 'SR1'
    M = int(args.method.split(' ')[1].strip('(').strip(')'))
    if len(args.method.split(' ')) > 2:
        red = {'EV': 'auto', '2N': 'tree', 'CT': 'm', 'SPL': 'split'}
        reduction = red[args.method.split(' ')[-1]]
    else:
        reduction = 'none'
else:
    method = args.method
    qnm, reduction, M = None, None, None

logger.debug('Running with CL method: %s' % method)

choose_regulator = {'EWC': baselines.EWC, 'MAS': baselines.MAS, 'OGD': baselines.OGD,
                    'LWF': baselines.LWF, 'CSQN': csqn.CSQN}
arguments = {'EWC': (shared_layers, n_classes), 'MAS': (shared_layers,), 'LWF': (),
             'OGD': (shared_layers, 200, n_classes),
             'CSQN': (qnm, shared_layers, M, reduction, n_classes)}


logger.info("")
logger.info("# 3 Experiments with CL with SQN Regularization")


for num in range(start_id, start_id + iterations):

    mod_list = [copy.deepcopy(net_one).state_dict()]

    logger.info("### RUN %d of %d " % (num, iterations))
    if method != 'Fine-Tuning':
        regulator = choose_regulator[method](*arguments[method])

    for task in range(n_tasks - 1):

        logger.info("Adapting Task %d to Task %d" % (task, task + 1))
        net_ = get_net()
        net_.load_state_dict(mod_list[-1])
        names, accs = [('Task %d' % i) for i in range(task + 1)], [round(test(net_, i, 'dev', print_result=False), 2)
                                                                   for i in range(task + 1)]
        logger.info('Initial model: ' + str(names) + " = " + str(accs))

        if method == 'EWC':
            regulator.compute_FIM(copy.deepcopy(net_), sample_data(task), task if shared_layers is not None else None)
        elif method == 'MAS':
            regulator.compute_IW(copy.deepcopy(net_), sample_data(task), task if shared_layers is not None else None)
        elif method == 'OGD':
            regulator.compute_gradients(copy.deepcopy(net_), train_data1(task),
                                        task if shared_layers is not None else None)
        elif method == 'LWF':
            regulator.set_old_net(net_)
            if shared_layers is not None:
                logger.debug('Method = LWF, running training non-shared layers..')
                train_net(net_, task + 1, epochs=n_epoch // 2, freeze_layers=shared_layers)
        elif method == 'CSQN':
            regulator.update(copy.deepcopy(net_), sample_data(task), task if shared_layers is not None else None)

        if not method in ['OGD']:

            logging.debug("Adapting without regularization..")
            net_ft = copy.deepcopy(net_)
            train_net(net_ft, task + 1, epochs=n_epoch)
            acc_ft = test(net_ft, task + 1, 'dev')
            if method == 'Fine-Tuning':
                mod_list.append(net_ft.state_dict())
                continue

            acc, alpha = 0, init_alpha[method]

            while acc < p_hyp * acc_ft:
                logger.info('[Learning task %d with alpha = %s]' % (task, str(alpha)))
                if method in ['EWC', 'MAS', 'CSQN']:
                    train_net(net_, task + 1, reg_loss=lambda x: regulator.regularize(x, alpha), epochs=n_epoch)
                else:
                    if shared_layers is None:
                        train_net(net_, task + 1,  reg_loss=lambda x, y: regulator.regularize(x, y, alpha), epochs=n_epoch)
                    else:
                        train_net(net_, task + 1, reg_loss=lambda x, y, t: regulator.regularize(x, y, alpha, t), epochs=n_epoch)
                acc = test(net_, task + 1, 'dev')
                logger.info("[Task %d] = [%.2f]" % (task + 1, acc))
                alpha = a_hyp * alpha

            best_alpha = alpha / a_hyp
            logger.info("Finished: best_alpha = %s - with accuracy of %.2f" % (str(best_alpha), acc))
            mod_list.append(net_.state_dict())
        else:
            train_net(net_, task + 1, grad_fn=lambda x: regulator.regularize(x), epochs=n_epoch, opt='sgd')
            mod_list.append(net_.state_dict())


    logger.info("## 3.3 Evaluation")
    net_list = []
    for mod in mod_list:
        net_ = get_net().cpu()
        net_.load_state_dict(mod)
        net_list.append(net_)
    if method == 'Fine-Tuning':
        test_and_update(net_list, method)
        break
    elif method == 'CSQN':
        test_and_update(net_list, regulator.get_name() + ' %d' % num)
    else:
        test_and_update(net_list, method + ' %d' % num)

