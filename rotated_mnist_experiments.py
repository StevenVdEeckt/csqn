# -*- coding: utf-8 -*-
"""Rotated_MNIST_experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BsYrz6Dx8s8ou1xDaByVYgalvS7fk9rk
"""

import torch
import logging
from baselines import *
from dataset import *
from train_functions import *
from csqn_inv import CSQN_Inv
import model
import argparse

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

parser = argparse.ArgumentParser(description='Rotated MNIST Baselines..')
parser.add_argument('--ewc', type=int, default=1, help='Indicates if EWC should be run (1) or not (0)')
parser.add_argument('--ogd', type=int, default=1, help='Indicates if OGD should be run (1) or not (0)')
parser.add_argument('--lwf', type=int, default=1, help='Indicates if LWF should be run (1) or not (0)')
parser.add_argument('--gpm', type=int, default=0, help='Indicates if GPM should be run (1) or not (0)')
parser.add_argument('--mas', type=int, default=1, help='Indicates if MAS should be run (1) or not (0)')
parser.add_argument('--cisqn', type=int, default=0, help='Indicates if CiSQN must be run (1) or not (0)')
parser.add_argument('--ft', type=int, default=0, help='Indicates if Fine-Tuning should be run (1) or not (0)')
parser.add_argument('--start', type=int, default=1, help='ID of first iteration')
parser.add_argument('--iter', type=int, default=3, help='Number of iterations to run each method')
parser.add_argument('--tasks', type=int, default=20, help='Number of tasks to run')
parser.add_argument('--angle', type=int, default=5, help="Angle of the rotation")
parser.add_argument('--log_id', type=int, default=0, help="ID of log file, to assure it is unique")

args = parser.parse_args()


""" The baselines and their settings """
run = []
if args.ft == 1:
    run.append('Fine-Tuning')
if args.ewc == 1:
    run.append('EWC')
if args.ogd == 1:
    run.append('OGD')
if args.lwf == 1:
    run.append('LWF')
if args.mas == 1:
    run.append('MAS')
if args.gpm == 1:
    run.append('GPM')
if args.cisqn == 1:
    run.append('CiSQN')
if len(run) == 0:
    raise Exception("Exception: no methods to run!")
start_id = {'EWC': args.start, 'MAS': args.start, 'OGD': args.start, 'LWF': args.start, 'GPM': args.start,
            'CiSQN': args.start, 'Fine-Tuning': 1}
iterations = {'EWC': args.iter, 'MAS': args.iter, 'OGD': args.iter, 'LWF': args.iter, 'GPM': args.iter,
              'CiSQN': args.iter, 'Fine-Tuning': 1}
init_alpha = {'EWC': 1e8, 'MAS': 1e5, 'LWF': 1e3}

""" Hyper-parameter search settings """
p_hyp = 0.95  # method must at least reach p*ACC_FT on new task
a_hyp = 0.50  # decaying factor

""" Experiment settings """
n_tasks = args.tasks
offset = args.angle
n_epoch = 10
batch_size = 64
first_task = 'task1_rotmnist_mlp'
res_name = 'rotated_mnist_results_%d_%d_lange' % (offset, n_tasks)
n_classes = 10

FORMAT = '%(asctime)-15s %(message)s'

logging.basicConfig(filename='rot_mnist_exp_%d_%d_%d.log' % (offset, n_tasks, args.log_id), level=logging.DEBUG, format=FORMAT, datefmt='%m/%d/%Y %I:%M:%S %p')
logger = logging.getLogger('main')

""" In case of GPM, we use a biasless model """
bias = not 'GPM' in run
if not bias:
    first_task += "_no_bias"


""" Load the data """
train_data, dev_data, test_data = get_rotated_mnist_data(n_tasks=n_tasks, angle=offset, batch_size=batch_size)
train_data1, _, _ = get_rotated_mnist_data(n_tasks=n_tasks, angle=offset, batch_size=1)
""" Load the network """
Net = lambda: model.get_net('mlp', n_classes, bias=bias)

net = Net()
logging.debug("Neural network contains " + str(torch.cat([p.view(-1) for p in list(net.parameters())]).numel())
      + " parameters.")
for n, p in net.state_dict().items():
    logging.debug(n + " ---> %d parameters" % (p.numel()))


try:
    results = torch.load(res_name, map_location='cpu')
except Exception as e:
    logger.debug('Exception when loading results: %s' % str(e))
    results = {}

init(train_data, dev_data, test_data, res_name, n_tasks, n_epoch)


net_one = Net()
try:
    net_one.load_state_dict(torch.load(first_task))
except Exception as e:
    logger.warning('Exception: file %s did not exist - training on task one.. (exception was = %s)' % (first_task, e))
    train_net(net_one, 0, epochs=n_epoch)
    torch.save(net_one.state_dict(), first_task)


logger.info("")
logger.info("# 3 Experiments with the Baselines")

for method in run:

    logger.info("Method: %s" % method)

    for num in range(start_id[method], start_id[method] + iterations[method]):

        mod_list = [copy.deepcopy(net_one.state_dict())]

        logger.info("### RUN %d of %d " % (num, iterations[method]))

        if method == 'EWC':
            regulator = EWC()
        elif method == 'MAS':
            regulator = MAS()
        elif method == 'LWF':
            regulator = LWF()
        elif method == 'OGD':
            regulator = OGD(M=200)
        elif method == 'GPM':
            regulator = GPM()
        elif method == 'CiSQN':
            regulator = CSQN_Inv(M=20)


        for task in range(n_tasks - 1):

            logger.info("Adapting Task %d to Task %d" % (task, task + 1))
            net_ = Net()
            net_.load_state_dict(mod_list[-1])
            names, accs = [('Task %d' % i) for i in range(task + 1)], [round(test(net_, i, 'dev', print_result=False), 2)
                                                                       for i in range(task + 1)]
            logger.info('Initial model: ' + str(names) + " = " + str(accs))

            if method == 'EWC':
                regulator.compute_FIM(net_, train_data(task))
                logger.debug(torch.norm(param_to_x(regulator.importance_weights)))
            elif method == 'MAS':
                regulator.compute_IW(net_, train_data(task))
            elif method == 'LWF':
                regulator.set_old_net(net_)
            elif method == 'OGD':
                regulator.compute_gradients(net_, train_data1(task))
            elif method == 'GPM':
                regulator.update(net_, train_data(task))
            elif method == 'CiSQN':
                regulator.update(net_, train_data(task))

            if not method in  ['OGD', 'GPM', 'CiSQN']:
                logger.debug("Adapting without regularization..")
                train_net(net_, task + 1, epochs=n_epoch)
                acc_ft = test(net_, task + 1, 'dev')
                if method == 'Fine-Tuning':
                    mod_list.append(net_.state_dict())
                    continue

                acc, alpha = 0, init_alpha[method]

                while acc < p_hyp * acc_ft:
                    logger.debug('alpha = %s' % alpha)
                    net_ = Net()
                    net_.load_state_dict(mod_list[-1])
                    if method == 'LWF':
                        train_net(net_, task + 1, reg_loss=lambda x, y: regulator.regularize(x, y, alpha), epochs=n_epoch)
                    else:
                        train_net(net_, task + 1, reg_loss=lambda x: regulator.regularize(x, alpha), epochs=n_epoch)
                    acc = test(net_, task + 1, 'dev')
                    logger.info("[Task %d] = [%.2f]" % (task + 1, acc))
                    alpha = a_hyp * alpha

                best_alpha = alpha / a_hyp
                logger.info("Best alpha was %d - with an average accuracy of %.2f" % (best_alpha, acc))

                mod_list.append(net_.state_dict())

            else:
                net_.load_state_dict(mod_list[-1])
                names, accs = [('Task %d' % i) for i in range(task + 1)], [
                    round(test(net_, i, 'dev', print_result=False), 2)
                    for i in range(task + 1)]
                logger.info('Initial model: ' + str(names) + " = " + str(accs))
                if method in ['OGD', 'CiSQN']:
                    train_net(net_, task + 1, grad_fn=lambda x: regulator.regularize(x), epochs=n_epoch, opt='sgd')
                elif method == 'GPM':
                    train_net(net_, task + 1, lay_grad_fn=lambda x, k: regulator.regularize(x, k), epochs=n_epoch, opt='sgd')
                mod_list.append(net_.state_dict())

        logger.info("## 3.3 Evaluation")
        net_list = []
        for mod in mod_list:
            net_ = Net().cpu()
            net_.load_state_dict(mod)
            net_list.append(net_)
        test_and_update(net_list, method + ' %d' % num)
