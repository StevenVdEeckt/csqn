# -*- coding: utf-8 -*-
"""Rotated_MNIST_experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BsYrz6Dx8s8ou1xDaByVYgalvS7fk9rk
"""

import torch
import logging
from baselines import *
from dataset import *
from train_functions import *
import model
import argparse

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

parser = argparse.ArgumentParser(description='Rotated MNIST Baselines..')
parser.add_argument('--ewc', type=int, default=1, help='Indicates if EWC should be run (1) or not (0)')
parser.add_argument('--ogd', type=int, default=1, help='Indicates if OGD should be run (1) or not (0)')
parser.add_argument('--lwf', type=int, default=1, help='Indicates if LWF should be run (1) or not (0)')
parser.add_argument('--gpm', type=int, default=1, help='Indicates if GPM should be run (1) or not (0)')
parser.add_argument('--mas', type=int, default=1, help='Indicates if MAS should be run (1) or not (0)')
parser.add_argument('--ft', type=int, default=0, help='Indicates if Fine-Tuning should be run (1) or not (0)')
parser.add_argument('--start', type=int, default=1, help='ID of first iteration')
parser.add_argument('--iter', type=int, default=3, help='Number of iterations to run each method')
parser.add_argument('--tasks', type=int, default=20, help='Number of tasks to run')
parser.add_argument('--angle', type=int, default=5, help="Angle of the rotation")
parser.add_argument('--log_id', type=int, default=0, help="ID of log file, to assure it is unique")

args = parser.parse_args()


""" The baselines and their settings """
run = []
if args.ft == 1:
    run.append('Fine-Tuning')
if args.ewc == 1:
    run.append('EWC')
if args.ogd == 1:
    run.append('OGD')
if args.lwf == 1:
    run.append('LWF')
if args.mas == 1:
    run.append('MAS')
if len(run) == 0:
    raise Exception("Exception: no methods to run!")
start_id = {'EWC': args.start, 'MAS': args.start, 'OGD': args.start, 'LWF': args.start, 'GPM': args.start, 'Fine-Tuning': 1}
iterations = {'EWC': args.iter, 'MAS': args.iter, 'OGD': args.iter, 'LWF': args.iter, 'GPM': args.iter, 'Fine-Tuning': 1}
lambdas = {'EWC': [1, 100, 1000, 10000, 100000],
           'MAS': [1, 10, 100, 1000, 10000],
           'LWF': [0.01, 0.1, 1, 10, 100]}
T_CV = 6
opt_alpha = {}


""" Experiment settings """
n_tasks = args.tasks
offset = args.angle
n_epoch = 10
batch_size = 64
first_task = 'task1_rotmnist_mlp'
res_name = 'rotated_mnist_results_%d_%d' % (offset, n_tasks)
n_classes = 10

FORMAT = '%(asctime)-15s %(message)s'

logging.basicConfig(filename='rot_mnist_exp_%d_%d_%d.log' % (offset, n_tasks, args.log_id), level=logging.DEBUG, format=FORMAT, datefmt='%m/%d/%Y %I:%M:%S %p')
logger = logging.getLogger('main')

""" In case of GPM, we use a biasless model """
bias = not 'GPM' in run
if not bias:
    first_task += "_no_bias"


""" Load the data """
train_data, dev_data, test_data = get_rotated_mnist_data(n_tasks=n_tasks, angle=offset, batch_size=batch_size)
train_data1, _, _ = get_rotated_mnist_data(n_tasks=n_tasks, angle=offset, batch_size=1)
""" Load the network """
Net = lambda: model.get_net('mlp', n_classes, bias=bias)

net = Net()
logging.debug("Neural network contains " + str(torch.cat([p.view(-1) for p in list(net.parameters())]).numel())
      + " parameters.")
for n, p in net.state_dict().items():
    logging.debug(n + " ---> %d parameters" % (p.numel()))


try:
    results = torch.load(res_name, map_location='cpu')
except Exception as e:
    logger.debug('Exception when loading results: %s' % str(e))
    results = {}

init(train_data, dev_data, test_data, res_name, n_tasks, n_epoch)


net_one = Net()
try:
    net_one.load_state_dict(torch.load(first_task))
except Exception as e:
    logger.warning('Exception: file %s did not exist - training on task one.. (exception was = %s)' % (first_task, e))
    train_net(net_one, 0, epochs=n_epoch)
    torch.save(net_one.state_dict(), first_task)


logger.info("")
logger.info("# 2 Hyper-parameter search")
for method in run:
    if method not in ['EWC', 'MAS', 'LWF']:
        continue

    logger.info("Method: %s" % method)
    best_avg, best_alpha = 0, 0

    if method == 'EWC':
        regulator = EWC()
    elif method == 'MAS':
        regulator = MAS()
    elif method == 'LWF':
        regulator = LWF()

    for alpha in lambdas[method]:

        net_ = Net()
        net_.load_state_dict(net_one.state_dict())

        for task in range(T_CV):

            logger.info("Adapting Task %d to Task %d" % (task, task + 1))

            if method == 'EWC':
                regulator.compute_FIM(net_, train_data(task))
            elif method == 'MAS':
                regulator.compute_IW(net_, train_data(task))
            elif method == 'LWF':
                regulator.set_old_net(net_)


            logger.debug('alpha = %s' % alpha)
            if method == 'LWF':
                train_net(net_, task + 1, reg_loss=lambda x, y: regulator.regularize(x, y, alpha),
                          epochs=n_epoch)
            else:
                train_net(net_, task + 1, reg_loss=lambda x: regulator.regularize(x, alpha), epochs=n_epoch)
        names, accs = [('Task %d' % i) for i in range(T_CV)], [round(test(net_, i, 'dev', print_result=False), 2)
                                                                   for i in range(T_CV)]
        logger.info('Final model: ' + str(names) + " = " + str(accs))
        avg_acc = sum(accs) / len(accs)
        if avg_acc > best_avg:
            best_avg = avg_acc
            best_alpha = alpha

    opt_alpha[method] = best_alpha

logger.info("")
logger.info("# 3 Experiments with the Baselines")

for method in run:

    logger.info("Method: %s" % method)

    alpha = opt_alpha[method]

    for num in range(start_id[method], start_id[method] + iterations[method]):

        mod_list = [copy.deepcopy(net_one.state_dict())]

        logger.info("### RUN %d of %d " % (num, iterations[method]))

        if method == 'EWC':
            regulator = EWC()
        elif method == 'MAS':
            regulator = MAS()
        elif method == 'LWF':
            regulator = LWF()
        elif method == 'OGD':
            regulator = OGD(M=200)
        elif method == 'GPM':
            regulator = GPM()


        for task in range(n_tasks - 1):

            logger.info("Adapting Task %d to Task %d" % (task, task + 1))
            net_ = Net()
            net_.load_state_dict(mod_list[-1])
            names, accs = [('Task %d' % i) for i in range(task + 1)], [round(test(net_, i, 'dev', print_result=False), 2)
                                                                       for i in range(task + 1)]
            logger.info('Initial model: ' + str(names) + " = " + str(accs))

            if method == 'EWC':
                regulator.compute_FIM(net_, train_data(task))
                logger.debug(torch.norm(param_to_x(regulator.importance_weights)))
            elif method == 'MAS':
                regulator.compute_IW(net_, train_data(task))
            elif method == 'LWF':
                regulator.set_old_net(net_)
            elif method == 'OGD':
                regulator.compute_gradients(net_, train_data1(task))
            elif method == 'GPM':
                regulator.update(net_, train_data(task))

            if not method in  ['OGD', 'GPM']:

                logger.debug('alpha = %s' % str(alpha))
                net_ = Net()
                net_.load_state_dict(mod_list[-1])
                if method == 'LWF':
                    train_net(net_, task + 1, reg_loss=lambda x, y: regulator.regularize(x, y, alpha), epochs=n_epoch)
                else:
                    train_net(net_, task + 1, reg_loss=lambda x: regulator.regularize(x, alpha), epochs=n_epoch)
                acc = test(net_, task + 1, 'dev')
                logger.info("[Task %d] = [%.2f]" % (task + 1, acc))

                mod_list.append(net_.state_dict())

            else:
                net_.load_state_dict(mod_list[-1])
                names, accs = [('Task %d' % i) for i in range(task + 1)], [
                    round(test(net_, i, 'dev', print_result=False), 2)
                    for i in range(task + 1)]
                logger.info('Initial model: ' + str(names) + " = " + str(accs))
                if method == 'OGD':
                    train_net(net_, task + 1, grad_fn=lambda x: regulator.regularize(x), epochs=n_epoch, opt='sgd')
                elif method == 'GPM':
                    train_net(net_, task + 1, lay_grad_fn=lambda x, k: regulator.regularize(x, k), epochs=n_epoch, opt='sgd')
                mod_list.append(net_.state_dict())

        logger.info("## 3.3 Evaluation")
        net_list = []
        for mod in mod_list:
            net_ = Net().cpu()
            net_.load_state_dict(mod)
            net_list.append(net_)
        test_and_update(net_list, method + ' %d' % num)
